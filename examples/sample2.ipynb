{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install langchain==0.0.167\n",
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.tools import BaseTool\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.agents import initialize_agent, Tool, tool\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "class LCTAGI():\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name = 'gpt-4'\n",
    "        ):\n",
    "        self.model_name = model_name\n",
    "\n",
    "    \n",
    "    def _generalize(self, input_prompt_):\n",
    "        print(\"> Generalize the input task.\")\n",
    "\n",
    "        generalize_llm = OpenAI(temperature=0,model_name = self.model_name)\n",
    "\n",
    "        prompt = \"\"\"\n",
    "        Please generalize the following sentences by replacing any numerical or other parts of the sentences with letters.\n",
    "        Output is only the text after rewriting.\n",
    "\n",
    "        ------------\n",
    "        {input}\n",
    "        ------------\n",
    "        \"\"\".format(input = input_prompt_)\n",
    "\n",
    "        responese = generalize_llm(prompt)\n",
    "\n",
    "        print(f'Generalized Task：{responese}')\n",
    "\n",
    "        return responese\n",
    "        \n",
    "\n",
    "    def _decide(self, input_prompt_, tools_):\n",
    "        print(\"> Determine if you should make a tool.\")\n",
    "        \n",
    "        decide_llm = OpenAI(temperature=0,model_name = self.model_name)\n",
    "\n",
    "        description_list = ''\n",
    "        for tool_ in tools_:\n",
    "            description = tool_.description\n",
    "            description_list += description + ','\n",
    "        description_list = '[' + description_list + ']'\n",
    "\n",
    "        prompt = \"\"\"\n",
    "        You are an agent that determines if the input task is executable. \n",
    "        All you can do is to be included in {exec_list}. \n",
    "        Answer \"Yes.\" if you can perform the task, or \"No.\" if you cannot.\n",
    "        \n",
    "        -----------\n",
    "        The entered task is:{input}\n",
    "        -----------\n",
    "        \"\"\".format(exec_list = description_list, input = input_prompt_)\n",
    "\n",
    "        responese = decide_llm(prompt)\n",
    "\n",
    "        if responese == \"Yes.\":\n",
    "            print('You do not need to create a tool to run it.')\n",
    "\n",
    "        elif responese == \"No.\":\n",
    "            print('You must create the tool before executing.')\n",
    "\n",
    "\n",
    "        return responese\n",
    "\n",
    "\n",
    "    def _tool_make(self, input_prompt_, folder_path_):\n",
    "        print(\"> Create a TOOL.\")\n",
    "\n",
    "        tool_llm = OpenAI(temperature=0, model_name = self.model_name)\n",
    "\n",
    "\n",
    "        create_prompt = \"\"\"\n",
    "        Create a python class that can execute {input} with a single string as input.\n",
    "        Output should be code only.\n",
    "        The following code was created with the input \"multiply two numbers\". Please create it like this code.\n",
    "        Do not enclose the output in ``python ``` or the like.\n",
    "        Do not change the class NewTool(BaseTool): in the code.\n",
    "        \n",
    "        ------------------\n",
    "        class NewTool(BaseTool):\n",
    "            name = \"MultiplicationTool\"\n",
    "            description = \"used for multiplication. The input is two numbers. For example, if you want to multiply 1 by 2, the input is '1,2'.\"\n",
    "\n",
    "            def _run(self, query: str) -> str:\n",
    "                \"Use the tool.\"\n",
    "                a, b = query.split(\",\")\n",
    "                c = int(a) * int(b)\n",
    "                result = c\n",
    "\n",
    "            return result \n",
    "\n",
    "            async def _arun(self, query: str) -> str:\n",
    "                \"Use the tool asynchronously.\"\n",
    "                raise NotImplementedError(\"BingSearchRun does not support async\")\n",
    "        ------------------\n",
    "        \"\"\".format(input = input_prompt_)\n",
    "\n",
    "        code = tool_llm(create_prompt)\n",
    "        #print('Created Code：\\n' + code)\n",
    "\n",
    "\n",
    "\n",
    "        name_prompt = \"\"\"\n",
    "        From the following code, extract the part written in \"\" after the name =.\n",
    "        The final output is only the content of the \"\".\n",
    "\n",
    "        ------------------\n",
    "        {code}\n",
    "        ------------------\n",
    "        \"\"\".format(code = code)\n",
    "\n",
    "        name = tool_llm(name_prompt)\n",
    "        print('Created tool name：' + name)\n",
    "\n",
    "        # Save to File\n",
    "        if folder_path_ != None:\n",
    "            with open(folder_path_ + f'{name}.py', mode='w') as file:\n",
    "                file.write(code)\n",
    "        \n",
    "\n",
    "        return code\n",
    "\n",
    "\n",
    "    def _execute(self,input_prompt_, tools_):\n",
    "        print(\"> Execute using the newly created tool.\")\n",
    "\n",
    "        excute_llm = OpenAI(temperature=0, model_name = self.model_name)\n",
    "\n",
    "        agent = initialize_agent(tools_, excute_llm, agent=\"zero-shot-react-description\", verbose=True)\n",
    "\n",
    "        agent.run(input_prompt_)\n",
    "\n",
    "        return\n",
    "\n",
    "\n",
    "    def run(self, input_prompt_, tools_, folder_path_ = None):\n",
    "\n",
    "        generalized_task = self._generalize(input_prompt_)\n",
    "    \n",
    "        output = self._decide(generalized_task, tools_)\n",
    "\n",
    "        if output == \"Yes.\":\n",
    "            self._execute(input_prompt_, tools_)\n",
    "\n",
    "        elif output == \"No.\":\n",
    "            new_tool_code = self._tool_make(generalized_task, folder_path_)\n",
    "\n",
    "            exec(new_tool_code, globals())\n",
    "            \n",
    "            new_tool = NewTool()           \n",
    "            tools_.append(new_tool)\n",
    "            \n",
    "            self._execute(input_prompt_, tools_)\n",
    "        \n",
    "        return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Generalize the input task.\n",
      "Generalized Task：Sum of prime numbers from A to B\n",
      "> Determine if you should make a tool.\n",
      "It is necessary to create a tool, so run it after creating the tool.\n",
      "> Create a TOOL.\n",
      "Created tool name：PrimeSumTool\n",
      "[NewTool(name='PrimeSumTool', description=\"used for calculating the sum of prime numbers between A and B. The input is a single string containing two numbers separated by a comma. For example, if you want to find the sum of prime numbers between 2 and 10, the input is '2,10'.\", args_schema=None, return_direct=False, verbose=False, callback_manager=<langchain.callbacks.shared.SharedCallbackManager object at 0xffff8c0b6c50>)]\n",
      "> Execute using the newly created tool.\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI need to find the sum of prime numbers between 1 and 50. I can use the PrimeSumTool for this.\n",
      "Action: PrimeSumTool\n",
      "Action Input: 1,50\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m328\u001b[0m\n",
      "Thought:Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3460, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_48659/4038601008.py\", line 8, in <module>\n",
      "    lctagi.run(input, tools, folder_path)\n",
      "  File \"/tmp/ipykernel_48659/1125205153.py\", line 164, in run\n",
      "    self._execute(input_prompt_, tools_)\n",
      "  File \"/tmp/ipykernel_48659/1125205153.py\", line 137, in _execute\n",
      "    agent.run(input_prompt_)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/langchain/chains/base.py\", line 213, in run\n",
      "    if self.memory is not None:\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/langchain/chains/base.py\", line 116, in __call__\n",
      "    only one param.\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/langchain/chains/base.py\", line 113, in __call__\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/langchain/agents/agent.py\", line 792, in _call\n",
      "    observation = InvalidTool().run(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/langchain/agents/agent.py\", line 672, in _take_next_step\n",
      "    return self.agent.input_keys\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/langchain/agents/agent.py\", line 384, in plan\n",
      "    return [\"output\"]\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/langchain/chains/llm.py\", line 151, in predict\n",
      "    {\"input_list\": input_list},\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/langchain/chains/base.py\", line 116, in __call__\n",
      "    only one param.\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/langchain/chains/base.py\", line 113, in __call__\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/langchain/chains/llm.py\", line 57, in _call\n",
      "    def output_keys(self) -> List[str]:\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/langchain/chains/llm.py\", line 118, in apply\n",
      "    async def aprep_prompts(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/langchain/chains/llm.py\", line 62, in generate\n",
      "    return [self.output_key]\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/langchain/llms/base.py\", line 107, in generate_prompt\n",
      "    run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/langchain/llms/base.py\", line 140, in generate\n",
      "    prompts: List[str],\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/langchain/llms/base.py\", line 137, in generate\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/langchain/llms/openai.py\", line 720, in _generate\n",
      "    params[\"stop\"] = stop\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/langchain/llms/openai.py\", line 102, in completion_with_retry\n",
      "    @retry_decorator\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/tenacity/__init__.py\", line 289, in wrapped_f\n",
      "    return self(f, *args, **kw)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/tenacity/__init__.py\", line 379, in __call__\n",
      "    do = self.iter(retry_state=retry_state)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/tenacity/__init__.py\", line 314, in iter\n",
      "    return fut.result()\n",
      "  File \"/opt/conda/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
      "    return self.__get_result()\n",
      "  File \"/opt/conda/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
      "    raise self._exception\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/tenacity/__init__.py\", line 382, in __call__\n",
      "    result = fn(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/langchain/llms/openai.py\", line 100, in _completion_with_retry\n",
      "    retry_decorator = _create_retry_decorator(llm)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 216, in request\n",
      "    result = self.request_raw(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/openai/api_requestor.py\", line 516, in request_raw\n",
      "    result = _thread_context.session.request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/requests/sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/requests/sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/requests/adapters.py\", line 489, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 449, in _make_request\n",
      "    six.raise_from(e, None)\n",
      "  File \"<string>\", line 3, in raise_from\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 444, in _make_request\n",
      "    httplib_response = conn.getresponse()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 1374, in getresponse\n",
      "    response.begin()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 318, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "  File \"/opt/conda/lib/python3.10/http/client.py\", line 279, in _read_status\n",
      "    line = str(self.fp.readline(_MAXLINE + 1), \"iso-8859-1\")\n",
      "  File \"/opt/conda/lib/python3.10/socket.py\", line 705, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "  File \"/opt/conda/lib/python3.10/ssl.py\", line 1274, in recv_into\n",
      "    return self.read(nbytes, buffer)\n",
      "  File \"/opt/conda/lib/python3.10/ssl.py\", line 1130, in read\n",
      "    return self._sslobj.read(len, buffer)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 2057, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 1288, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 1177, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 1030, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 960, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(record))\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 870, in format_record\n",
      "    frame_info.lines, Colors, self.has_colors, lvals\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 704, in lines\n",
      "    return self._sd.lines\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/stack_data/utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/stack_data/core.py\", line 734, in lines\n",
      "    pieces = self.included_pieces\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/stack_data/utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/stack_data/core.py\", line 681, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/stack_data/utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/stack_data/core.py\", line 660, in executing_piece\n",
      "    return only(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/executing/executing.py\", line 190, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "input = 'Sum of prime numbers from 1 to 50'\n",
    "\n",
    "folder_path = '/home/langchain-tools/data/'\n",
    "tools = []\n",
    "\n",
    "lctagi = LCTAGI()\n",
    "\n",
    "lctagi.run(input, tools, folder_path)\n",
    "\n",
    "#lctagi.run(input, tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.9 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
