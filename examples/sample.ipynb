{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LCT-AGI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install langchain\n",
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.agents import initialize_agent, Tool, tool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 参照するLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input = 'trainig XGBoost'\n",
    "input = 'training LightGBM'\n",
    "exec_list = [\n",
    "    'training LightGBM'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deciding_agent(input_, exec_list_):\n",
    "    print(\"deciding agent\")\n",
    "    \n",
    "    decide_llm = OpenAI(temperature=0,model_name=\"gpt-4\")\n",
    "\n",
    "    i = ''\n",
    "    for i in exec_list:\n",
    "        i = i + ','\n",
    "    i = '[' + i + ']'\n",
    "\n",
    "    prompt = \"\"\"\n",
    "    You are an agent that determines if the input task is executable. \n",
    "    All you can do is to be included in {exec_list}. \n",
    "    Answer \"Yes\" if you can perform the task, or \"No\" if you cannot.\n",
    "    ------\n",
    "    The entered task is:{input}\n",
    "    ------\n",
    "    \"\"\".format(exec_list = i,input = input_)\n",
    "\n",
    "    return decide_llm(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/langchain/llms/openai.py:165: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/langchain/llms/openai.py:672: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Yes'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decide_agent(input, exec_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 道具をつくるLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toolmaking_agent(input_):\n",
    "    print(\"toolmaking agent\")\n",
    "\n",
    "    toolmaking_llm = OpenAI(temperature=0, model_name=\"gpt-4\")\n",
    "\n",
    "\n",
    "    prompt = \"\"\"\n",
    "    Create a python function that can execute {input} with a single string as input.\n",
    "    The code below is the code created with the input \"multiply two numbers\".\n",
    "    The output is only the code of the created python.\n",
    "    -----------------------------------\n",
    "    @tool('new_tool')\n",
    "    def new_tool(query: str) -> str:\n",
    "        'Used for multiplication. The input is two numbers. For example, if you want to multiply 1 by 2, the input is '1,2'.'\n",
    "        a, b = query.split(\",\")\n",
    "        c = int(a) * int(b)\n",
    "        return c\n",
    "    \"\"\".format(input = input_)\n",
    "\n",
    "    code = toolmaking_llm(prompt)\n",
    "\n",
    "    #name = query.replace(' ','_').lower()\n",
    "\n",
    "    #bos = \"\\n@tool('{name}_tool)\\n\"\\\n",
    "    #        \"def {name}_tool(query: str) -> str:\\n\"\\\n",
    "    #        \"    'useful for {query}'\"\n",
    "    \n",
    "    #code = textwrap.indent(textwrap.dedent(code)[:], '    ')\n",
    "\n",
    "    #eos = \"\\n    result = 'finish {name}\\n\"\\\n",
    "    #        \"    return result\" \n",
    "\n",
    "    #result = bos.format(name=name, query=query) + code + eos.format(name=name)\n",
    "\n",
    "    file = open('/home/langchain-tools/examples/new_tool.py', mode='w')\n",
    "    file.write(code)\n",
    "    file.close()\n",
    "\n",
    "    return 'finish'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toolmaking agent\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'finish'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = 'Output multiples of 3 up to 20'\n",
    "toolmaking_agent(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 実行するLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def executing_agent(input_, tools_):\n",
    "    print(\"executing agent\")\n",
    "\n",
    "    excute_llm = OpenAI(temperature=0, model_name=\"gpt-4\")\n",
    "\n",
    "    agent = initialize_agent(tools_, excute_llm, agent=\"zero-shot-react-description\", verbose=True)\n",
    "\n",
    "    agent.run(input_)\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 全体"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(input_, exec_list_, tools_):\n",
    "    \n",
    "    output = deciding_agent(input_, exec_list_)\n",
    "\n",
    "    if output == \"Yes\":\n",
    "        print('Execute it as it is executable without creating a tool.')\n",
    "        executing_agent(input_, tools_)\n",
    "\n",
    "    elif output == \"No\":\n",
    "        print('It is necessary to create a tool, so run it after creating the tool.')\n",
    "        toolmaking_agent(input_)\n",
    "\n",
    "        tools.append(Tool(name = f'{new_tool}', func = new_tool.new_tool, description=''))\n",
    "\n",
    "        executing_agent(input_, tools_)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/langchain/llms/openai.py:165: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/langchain/llms/openai.py:672: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ある日、カエルとヘビが一緒に映画を見に行きました。映画が始まると、カエルはポップコーンを食べ始めました。ヘビはカエルを見て言いました、「君は本当にポップコーンが好きだね。でも、どうして飛び跳ねないの？」カエルは笑って答えました、「それは、ポップコーンが私のお腹の中に入ると、もうポップしなくなるからさ！」'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = OpenAI(temperature=0,model_name=\"gpt-4\")\n",
    "llm(\"面白いことを言って\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.9 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
